{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATTENTION U-NET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQYrJTTtm11LgHIxTh5Fk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeeramVineethReddy/Geno_Intern/blob/master/ATTENTION_U_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5QdqmSqOCzT"
      },
      "source": [
        "from __future__ import division, print_function\n",
        "from collections import defaultdict\n",
        "import os, pickle, sys\n",
        "import shutil\n",
        "from functools import partial\n",
        "\n",
        "import cv2\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "#!pip install scipy\n",
        "#!pip install misc\n",
        "from scipy.misc import imresize\n",
        "from skimage.transform import resize\n",
        "from skimage.exposure import equalize_adapthist, equalize_hist\n",
        "\n",
        "# from models import *\n",
        "# from metrics import dice_coef, dice_coef_loss\n",
        "from augmenters import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import utils\n",
        "from keras.callbacks import TensorBoard\n",
        "import newmodels\n",
        "import losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG9r4NBoYPCu"
      },
      "source": [
        "#load data function\n",
        "\n",
        "def load_data():\n",
        "\n",
        "    X_train = np.load('../data/X_train.npy')\n",
        "    y_train = np.load('../data/y_train.npy')\n",
        "    X_val = np.load('../data/X_val.npy')\n",
        "    y_val = np.load('../data/y_val.npy')\n",
        "\n",
        "\n",
        "    return X_train, y_train, X_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhXAYNdVQ8qj"
      },
      "source": [
        "#image_resize\n",
        "\n",
        "def img_resize(imgs, img_rows, img_cols, equalize=True):\n",
        "\n",
        "    new_imgs = np.zeros([len(imgs), img_rows, img_cols])\n",
        "    for mm, img in enumerate(imgs):\n",
        "        if equalize:\n",
        "            img = equalize_adapthist( img, clip_limit=0.05 )\n",
        "            # img = clahe.apply(cv2.convertScaleAbs(img))\n",
        "\n",
        "        new_imgs[mm] = cv2.resize( img, (img_rows, img_cols), interpolation=cv2.INTER_NEAREST )\n",
        "\n",
        "    return new_imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhH9cYQjX-4-"
      },
      "source": [
        "#data converting to array\n",
        "\n",
        "\n",
        "def data_to_array(img_rows, img_cols):\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=0.05, tileGridSize=(int(img_rows/8),int(img_cols/8)) )\n",
        "\n",
        "    fileList =  os.listdir('../data/train/')\n",
        "\n",
        "    fileList = filter(lambda x: '.mhd' in x, fileList)\n",
        "    fileList = list(fileList)\n",
        "    fileList.sort()\n",
        "\n",
        "    val_list = [5,15,25,35,45]\n",
        "    train_list = list( set(range(50)) - set(val_list) )\n",
        "    count = 0\n",
        "    for the_list in [train_list,  val_list]:\n",
        "        images = []\n",
        "        masks = []\n",
        "\n",
        "        filtered = filter(lambda x: any(str(ff).zfill(2) in x for ff in the_list), fileList)\n",
        "\n",
        "        for filename in filtered:\n",
        "\n",
        "            itkimage = sitk.ReadImage('../data/train/'+filename)\n",
        "            imgs = sitk.GetArrayFromImage(itkimage)\n",
        "\n",
        "            if 'segm' in filename.lower():\n",
        "                imgs= img_resize(imgs, img_rows, img_cols, equalize=False)\n",
        "                masks.append( imgs )\n",
        "\n",
        "            else:\n",
        "                imgs = img_resize(imgs, img_rows, img_cols, equalize=True)\n",
        "                images.append(imgs )\n",
        "\n",
        "        images = np.concatenate( images , axis=0 ).reshape(-1, img_rows, img_cols, 1)\n",
        "        masks = np.concatenate(masks, axis=0).reshape(-1, img_rows, img_cols, 1)\n",
        "        masks = masks.astype(int)\n",
        "\n",
        "        #Smooth images using CurvatureFlow\n",
        "        images = smooth_images(images)\n",
        "\n",
        "        if count==0:\n",
        "            mu = np.mean(images)\n",
        "            sigma = np.std(images)\n",
        "            images = (images - mu)/sigma\n",
        "\n",
        "            np.save('../data/X_train.npy', images)\n",
        "            np.save('../data/y_train.npy', masks)\n",
        "        elif count==1:\n",
        "            images = (images - mu)/sigma\n",
        "\n",
        "            np.save('../data/X_val.npy', images)\n",
        "            np.save('../data/y_val.npy', masks)\n",
        "        count+=1\n",
        "\n",
        "    fileList =  os.listdir('../data/test/')\n",
        "    fileList = filter(lambda x: '.mhd' in x, fileList)\n",
        "    fileList = list(fileList)\n",
        "    fileList.sort()\n",
        "    n_imgs=[]\n",
        "    images=[]\n",
        "    for filename in fileList:\n",
        "        itkimage = sitk.ReadImage('../data/test/'+filename)\n",
        "        imgs = sitk.GetArrayFromImage(itkimage)\n",
        "        imgs = img_resize(imgs, img_rows, img_cols, equalize=True)\n",
        "        images.append(imgs)\n",
        "        n_imgs.append( len(imgs) )\n",
        "\n",
        "    images = np.concatenate( images , axis=0 ).reshape(-1, img_rows, img_cols, 1)\n",
        "    images = smooth_images(images)\n",
        "    images = (images - mu)/sigma\n",
        "    np.save('../data/X_test.npy', images)\n",
        "    np.save('../data/test_n_imgs.npy', np.array(n_imgs) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXwoZfGJY4Jt"
      },
      "source": [
        "# augment validation data\n",
        "def augment_validation_data(X_train, y_train, seed=10):\n",
        "\n",
        "    img_rows = X_train.shape[1]\n",
        "    img_cols =  X_train.shape[2]\n",
        "\n",
        "    x, y = np.meshgrid(np.arange(img_rows), np.arange(img_cols), indexing='ij')\n",
        "    elastic = partial(elastic_transform, x=x, y=y, alpha=img_rows*1.5, sigma=img_rows*0.07 )\n",
        "    # we create two instances with the same arguments\n",
        "    data_gen_args = dict(preprocessing_function=elastic)\n",
        "\n",
        "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "    image_datagen.fit(X_train, seed=seed)\n",
        "    mask_datagen.fit(y_train, seed=seed)\n",
        "\n",
        "    image_generator = image_datagen.flow(X_train, batch_size=100, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=100, seed=seed)\n",
        "\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "\n",
        "    count=0\n",
        "    X_val = []\n",
        "    y_val = []\n",
        "\n",
        "    for X_batch, y_batch in train_generator:\n",
        "\n",
        "        if count==5:\n",
        "            break\n",
        "\n",
        "        count+=1\n",
        "\n",
        "        X_val.append(X_batch)\n",
        "        y_val.append(y_batch)\n",
        "\n",
        "    X_val = np.concatenate(X_val, axis=0)\n",
        "    y_val = np.concatenate(y_val, axis=0)\n",
        "    return X_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgG_qEuEZWjh"
      },
      "source": [
        "def keras_fit_generator_attention(img_rows=96, img_cols=96, n_imgs=10**4, batch_size=32, regenerate=True, model_type = 'unet', loss_type='dice', train=True, test=True):\n",
        "\n",
        "    if regenerate:\n",
        "        data_to_array(img_rows, img_cols)\n",
        "        #preprocess_data()\n",
        "\n",
        "    X_train, y_train, X_val, y_val = load_data()\n",
        "    img_rows = X_train.shape[1]\n",
        "    img_cols =  X_train.shape[2]\n",
        "\n",
        "    \n",
        "    gt1 = y_train[:,::8,::8,:]\n",
        "    gt2 = y_train[:,::4,::4,:]\n",
        "    gt3 = y_train[:,::2,::2,:]\n",
        "    gt4 = y_train\n",
        "    gt_train = [gt1,gt2,gt3,gt4]\n",
        "    \n",
        "    #choose loss function\n",
        "    if loss_type == 'dice':\n",
        "        loss_f = losses.dice_loss\n",
        "    elif loss_type =='tversky':\n",
        "        loss_f = losses.tversky_loss\n",
        "    elif loss_type =='focal_tversky':\n",
        "        loss_f = losses.focal_tversky\n",
        "    else:\n",
        "        print('wrong loss function type')\n",
        "        return -1\n",
        "    \n",
        "    plot_type = 0\n",
        "    epochs_num = 50\n",
        "    model_name = model_type+'_'+loss_type\n",
        "    filepath='../data/weights/weights_'+model_type+'_'+loss_type+'.hdf5'  #you need to create this folder before run\n",
        "    result_text_path='../data/results/results.txt'  #you need to create this file before run\n",
        "    result_images_path='../data/results/images1/'   #you need to create this folder before run\n",
        "    #choose model\n",
        "    if model_type=='unet':\n",
        "        sgd = SGD(lr=0.01, momentum=0.90)\n",
        "        model = newmodels.unet(sgd, (256,256,1), loss_f)\n",
        "        model_checkpoint = ModelCheckpoint(filepath, monitor='val_dsc', \n",
        "                             verbose=1, save_best_only=True, \n",
        "                             save_weights_only=True, mode='max')\n",
        "    elif model_type=='attn_unet':\n",
        "        sgd = SGD(lr=0.01, momentum=0.90, decay=1e-6)\n",
        "        model = newmodels.attn_unet(sgd, (256,256,1), loss_f)\n",
        "        model_checkpoint = ModelCheckpoint(filepath, monitor='val_dsc', \n",
        "                             verbose=1, save_best_only=True, \n",
        "                             save_weights_only=True, mode='max')\n",
        "    elif model_type=='ds_mi_attn_unet':\n",
        "        plot_type = 1\n",
        "        y_train = gt_train\n",
        "        sgd = SGD(lr=0.01, momentum=0.90, decay=1e-6)\n",
        "        model = newmodels.attn_reg(sgd,(256,256,1),loss_f)\n",
        "        model_checkpoint = ModelCheckpoint(filepath, monitor='val_final_dsc', \n",
        "                             verbose=1, save_best_only=True, \n",
        "                             save_weights_only=True, mode='max')\n",
        "    else:\n",
        "        print('wrong model type')\n",
        "        return -1\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    c_backs = [model_checkpoint]\n",
        "    c_backs.append( EarlyStopping(monitor='loss', min_delta=0.001, patience=5) )\n",
        "    \n",
        "    \n",
        "    model_name = (model_type+'_'+loss_type+'{}').format(int(time.time()))\n",
        "    tb_call_back = TensorBoard(log_dir='./log_dir_5.25.1/{}'.format(model_name))\n",
        "    c_backs.append(tb_call_back)\n",
        "\n",
        "    if train:\n",
        "        hist = model.fit(X_train, y_train, validation_split=0.15,\n",
        "                     shuffle=True, epochs=epochs_num, batch_size=batch_size,\n",
        "                     verbose=True, callbacks=c_backs)#, callbacks=[estop,tb])\n",
        "\n",
        "        h = hist.history\n",
        "#         utils.plot(h, epochs_num, batch_size, img_cols, plot_type, model_name = model_name)\n",
        "\n",
        "    if test==True:\n",
        "        X_val = np.load('../data/X_val.npy')\n",
        "        y_val = np.load('../data/y_val.npy')\n",
        "        num_test = X_val.shape[0]\n",
        "        test_img_list = os.listdir('../data/test/')\n",
        "        if model_type=='ds_mi_attn_unet':\n",
        "            _,_,_,preds = model.predict(X_val)\n",
        "        else:\n",
        "            preds = model.predict(X_val)   #use this if the model is not muti-input ds unet\n",
        "        \n",
        "        preds_up=[]\n",
        "        dsc = np.zeros((num_test,1))\n",
        "        recall = np.zeros_like(dsc)\n",
        "        tn = np.zeros_like(dsc)\n",
        "        prec = np.zeros_like(dsc)\n",
        "\n",
        "        thresh = 0.5\n",
        "        \n",
        "        \n",
        "        # check the predictions from the trained model \n",
        "        for i in range(num_test):\n",
        "            gt = y_val[i]\n",
        "            pred_up = cv2.resize(preds[i], (gt.shape[1], gt.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "            preds_up.append(pred_up)\n",
        "            dsc[i] = utils.check_preds(pred_up > thresh, gt)\n",
        "            recall[i], _, prec[i] = utils.auc(gt, pred_up >thresh)\n",
        "\n",
        "        f = open(result_text_path, \"a\")\n",
        "        f.write('\\n')\n",
        "        f.write('-'*30)\n",
        "        f.write('\\nModel name: ')\n",
        "        f.write(model_name)\n",
        "        f.write('\\n DSC \\t\\t{0:^.3f} \\n Recall \\t{1:^.3f} \\n Precision\\t{2:^.3f}'.format(\n",
        "                np.sum(dsc)/num_test,  \n",
        "                np.sum(recall)/num_test,\n",
        "                np.sum(prec)/num_test ))\n",
        "        f.write('\\n')\n",
        "        f.close()\n",
        "        \n",
        "        \n",
        "        print('-'*30)\n",
        "        print('At threshold =', thresh)\n",
        "        print('\\n DSC \\t\\t{0:^.3f} \\n Recall \\t{1:^.3f} \\n Precision\\t{2:^.3f}'.format(\n",
        "                np.sum(dsc)/num_test,  \n",
        "                np.sum(recall)/num_test,\n",
        "                np.sum(prec)/num_test ))\n",
        "\n",
        "        # check the predictions with the best saved model from checkpoint\n",
        "        model.load_weights(filepath)\n",
        "    \n",
        "        if model_type=='ds_mi_attn_unet':\n",
        "            _,_,_,preds = model.predict(X_val)\n",
        "        else:\n",
        "            preds = model.predict(X_val)   #use this if the model is not muti-input ds unet\n",
        "\n",
        "        preds_up=[]\n",
        "        dsc = np.zeros((num_test,1))\n",
        "        recall = np.zeros_like(dsc)\n",
        "        tn = np.zeros_like(dsc)\n",
        "        prec = np.zeros_like(dsc)\n",
        "\n",
        "        for i in range(num_test):\n",
        "            gt = y_val[i]\n",
        "            pred_up = cv2.resize(preds[i], (gt.shape[1], gt.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "            preds_up.append(pred_up)\n",
        "            dsc[i] = utils.check_preds(pred_up > thresh, gt)\n",
        "            recall[i], _, prec[i] = utils.auc(gt, pred_up >thresh)\n",
        "\n",
        "        print('-'*30)\n",
        "        print('USING HDF5 saved model at thresh=', thresh)\n",
        "        print('\\n DSC \\t\\t{0:^.3f} \\n Recall \\t{1:^.3f} \\n Precision\\t{2:^.3f}'.format(\n",
        "                np.sum(dsc)/num_test,  \n",
        "                np.sum(recall)/num_test,\n",
        "                np.sum(prec)/num_test ))\n",
        "        \n",
        "        f = open(result_text_path, \"a\")\n",
        "        f.write('\\n')\n",
        "        f.write('-'*30)\n",
        "        f.write('\\nModel name: ')\n",
        "        f.write(model_name)\n",
        "        f.write('\\nUSING HDF5 saved model')\n",
        "        f.write('\\n DSC \\t\\t{0:^.3f} \\n Recall \\t{1:^.3f} \\n Precision\\t{2:^.3f}'.format(\n",
        "                np.sum(dsc)/num_test,  \n",
        "                np.sum(recall)/num_test,\n",
        "                np.sum(prec)/num_test ))\n",
        "        f.write('\\n')\n",
        "        f.close()\n",
        "\n",
        "        while True:\n",
        "            idx = np.random.randint(0,num_test)\n",
        "            if utils.avg_img(y_val[idx]>0)>3 and utils.avg_img(y_val[idx]>0) <8:\n",
        "#                 print(utils.avg_img(y_val[idx]>0))\n",
        "                break\n",
        "\n",
        "        idxs = [94,55,69,75,52,77]\n",
        "        \n",
        "        for idx in idxs:\n",
        "            #plot a test sample for each model\n",
        "            gt_plot = y_val[idx]\n",
        "            plt.figure(dpi=200)\n",
        "            plt.subplot(121)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.squeeze(gt_plot), cmap='gray')\n",
        "            plt.title('Original Segmentated Img {}'.format(idx))\n",
        "            plt.subplot(122)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.squeeze(preds_up[idx]), cmap='gray')\n",
        "            plt.title('Mask {}'.format(idx))\n",
        "\n",
        "            plt.savefig(result_images_path+str(idx)+'/'+model_name+'ori-gt-'.format('.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "LNiTWvxqZ9cw",
        "outputId": "2259d7b6-3031-4819-f381-d1e19ae534c9"
      },
      "source": [
        "if __name__=='__main__':\n",
        "\n",
        "    CUDA_CACHE_PATH='~/yining/CUDA_CACHE'\n",
        "    import time\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    model_types = ['unet','attn_unet','ds_mi_attn_unet']\n",
        "    loss_types = ['dice','tversky','focal_tversky']\n",
        "    \n",
        "    for i in range(3):\n",
        "#         j=1\n",
        "        for j in range(3):\n",
        "            keras_fit_generator_attention(img_rows=256, img_cols=256, regenerate=False,model_type = model_types[i], loss_type=loss_types[j], train = False,test = True,n_imgs=15*10**4, batch_size=16)\n",
        "\n",
        "    \n",
        "#     X_train, y_train, X_val, y_val = load_data()\n",
        "#     y_total = np.append(y_train,y_val,axis=0)\n",
        "#     por = 0\n",
        "    \n",
        "#     for idx in range(y_total.shape[0]):\n",
        "#         por += utils.avg_img(y_total[idx]>0)\n",
        "    \n",
        "#     por /= y_total.shape[0]\n",
        "#     print(por)\n",
        "    \n",
        "    end = time.time()\n",
        "\n",
        "    print('Elapsed time:', round((end-start)/60, 2 ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-491ddb599a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         j=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mkeras_fit_generator_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregenerate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_imgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-fb0174f44f6b>\u001b[0m in \u001b[0;36mkeras_fit_generator_attention\u001b[0;34m(img_rows, img_cols, n_imgs, batch_size, regenerate, model_type, loss_type, train, test)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#preprocess_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimg_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimg_cols\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-23e16eb3e8d0>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/X_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/y_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/X_val.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/X_train.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwToSeDuaCsO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}